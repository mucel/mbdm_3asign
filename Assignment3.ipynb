{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ad96fa1",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b18233d",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "392605b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary libraries\n",
    "\n",
    "from mesa import Agent, Model\n",
    "from mesa.time import SimultaneousActivation\n",
    "from mesa.space import NetworkGrid\n",
    "from mesa.datacollection import DataCollector\n",
    "import networkx as nx\n",
    "import numpy as np \n",
    "import random\n",
    "from typing import Iterable, List, Dict\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from typing import Callable, Dict, Optional, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fc7d78",
   "metadata": {},
   "source": [
    "## Strategy selection helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c13bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Strategy selection helpers\n",
    "#\n",
    "# We provide two different ways agents can choose their strategy:\n",
    "# - `choose_strategy_imitate`: choose strategy of the highest-payoff neighbour (including self).\n",
    "# - `choose_strategy_logit`: choose strategy using logit / softmax choice.\n",
    "#\n",
    "####################################\n",
    "def choose_strategy_imitate(agent, neighbors):\n",
    "    \"\"\"Choose strategy of the highest-payoff neighbour (including self).\"\"\"\n",
    "    candidates = neighbors + [agent]\n",
    "    best = max(candidates, key=lambda a: a.payoff)\n",
    "    return best.strategy\n",
    "\n",
    "\n",
    "def choose_strategy_logit(agent, neighbors, a_I, b, tau):\n",
    "    \"\"\"Choose strategy using logit / softmax choice.\n",
    "\n",
    "    Parameters\n",
    "    - agent: the agent choosing a strategy\n",
    "    - neighbors: list of neighbour agents\n",
    "    - a_I: effective coordination payoff given current infrastructure\n",
    "    - b: defection payoff\n",
    "    - tau: temperature parameter for softmax\n",
    "    \"\"\"\n",
    "    # compute expected payoffs for C and D\n",
    "    pi_C = 0.0\n",
    "    pi_D = 0.0\n",
    "    for other in neighbors:\n",
    "        s_j = other.strategy\n",
    "        if s_j == \"C\":\n",
    "            pi_C += a_I\n",
    "            pi_D += b\n",
    "        else:\n",
    "            pi_C += 0.0\n",
    "            pi_D += b\n",
    "\n",
    "    # softmax choice\n",
    "    denom = np.exp(pi_C / tau) + np.exp(pi_D / tau)\n",
    "    P_C = np.exp(pi_C / tau) / denom if denom > 0 else 0.5\n",
    "    return \"C\" if random.random() < P_C else \"D\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5218c4ce",
   "metadata": {},
   "source": [
    "## Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0c5a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Agent class\n",
    "#\n",
    "# The EVAgent class implements the single agent at a graph node.\n",
    "#\n",
    "# Attributes\n",
    "# - strategy: \"C\" (adopt EV) or \"D\" (defect / ICE)\n",
    "# - payoff: accumulated payoff from interactions with neighbours\n",
    "# - next_strategy: strategy chosen for the next time step\n",
    "####################################\n",
    "class EVAgent(Agent):\n",
    "    \"\"\"Single agent at a graph node.\n",
    "\n",
    "    Attributes\n",
    "    - strategy: \"C\" (adopt EV) or \"D\" (defect / ICE)\n",
    "    - payoff: accumulated payoff from interactions with neighbours\n",
    "    - next_strategy: strategy chosen for the next time step\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, unique_id, model, init_strategy=\"D\"):\n",
    "        super().__init__(unique_id, model)\n",
    "        self.strategy = init_strategy\n",
    "        self.payoff = 0.0\n",
    "        self.next_strategy = init_strategy\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Compute payoff from interactions with neighbours.\n",
    "\n",
    "        Stag Hunt payoff rules:\n",
    "        - C vs C: `a_I` (coordination enhanced by infrastructure)\n",
    "        - C vs D: 0\n",
    "        - D vs C: `b`\n",
    "        - D vs D: `b`\n",
    "        \"\"\"\n",
    "        I = self.model.infrastructure\n",
    "        a0 = self.model.a0\n",
    "        beta_I = self.model.beta_I\n",
    "        b = self.model.b\n",
    "        a_I = a0 + beta_I * I\n",
    "\n",
    "        neighbor_agents = []\n",
    "        for nbr in self.model.G.neighbors(self.pos):\n",
    "            neighbor_agents.extend(self.model.grid.get_cell_list_contents([nbr]))\n",
    "        if not neighbor_agents:\n",
    "            self.payoff = 0.0\n",
    "            return\n",
    "\n",
    "        payoff = 0.0\n",
    "        for other in neighbor_agents:\n",
    "            s_i = self.strategy\n",
    "            s_j = other.strategy\n",
    "            if s_i == \"C\" and s_j == \"C\":\n",
    "                payoff += a_I\n",
    "            elif s_i == \"C\" and s_j == \"D\":\n",
    "                payoff += 0.0\n",
    "            elif s_i == \"D\" and s_j == \"C\":\n",
    "                payoff += b\n",
    "            else:\n",
    "                payoff += b\n",
    "        self.payoff = payoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280808af",
   "metadata": {},
   "source": [
    "## Advance method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ada6ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ####################################\n",
    "    # Advance method\n",
    "    #\n",
    "    # The advance method updates the agent's strategy based on the selected rule.\n",
    "    #\n",
    "    # Parameters\n",
    "    # - strategy_choice_func: the strategy selection function to use (\"imitate\" or \"logit\")\n",
    "    ####################################\n",
    "    def advance(self, strategy_choice_func=\"imitate\"):\n",
    "        \"\"\"Update next_strategy using the selected rule.\n",
    "\n",
    "        If called without an explicit rule, read `self.model.strategy_choice_func`.\n",
    "        Commit `self.strategy = self.next_strategy` for synchronous updates.\n",
    "        \"\"\"\n",
    "        func = strategy_choice_func if strategy_choice_func is not None else getattr(self.model, \"strategy_choice_func\", \"imitate\")\n",
    "\n",
    "        neighbor_agents = []\n",
    "        for nbr in self.model.G.neighbors(self.pos):\n",
    "            neighbor_agents.extend(self.model.grid.get_cell_list_contents([nbr]))\n",
    "\n",
    "        if func == \"imitate\":\n",
    "            self.next_strategy = choose_strategy_imitate(self, neighbor_agents)\n",
    "        elif func == \"logit\":\n",
    "            a_I = self.model.a0 + self.model.beta_I * self.model.infrastructure\n",
    "            self.next_strategy = choose_strategy_logit(self, neighbor_agents, a_I, self.model.b, getattr(self.model, \"tau\", 1.0))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy choice function: {func}\")\n",
    "\n",
    "        self.strategy = self.next_strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee45d8e6",
   "metadata": {},
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f52a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ####################################\n",
    "    # Model class\n",
    "    #\n",
    "    # The EVStagHuntModel class implements the Mesa model for EV Stag Hunt on a network.\n",
    "    #\n",
    "    # Parameters\n",
    "    # - initial_ev: number of initial EV nodes\n",
    "    # - a0: base payoff for EV adoption\n",
    "    # - beta_I: payoff enhancement factor for EV adoption\n",
    "    # - b: payoff for ICE defection\n",
    "    # - g_I: infrastructure growth rate\n",
    "    # - I0: initial infrastructure level\n",
    "    # - seed: random seed for reproducibility\n",
    "    # - network_type: type of network to generate (\"random\" or \"BA\")\n",
    "    # - n_nodes: number of nodes in the network\n",
    "    # - p: probability of edge creation in random network\n",
    "    # - m: number of edges to attach to new node in BA network\n",
    "    # - collect: whether to collect agent and model-level data\n",
    "    # - strategy_choice_func: strategy selection function (\"imitate\" or \"logit\")\n",
    "    # - tau: temperature parameter for softmax choice (only used with \"logit\")\n",
    "    ####################################\n",
    "#\n",
    "class EVStagHuntModel(Model):\n",
    "        \"\"\"Mesa model for EV Stag Hunt on a network.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_ev=10,\n",
    "        a0=2.0,\n",
    "        beta_I=3.0,\n",
    "        b=1.0,\n",
    "        g_I=0.1,\n",
    "        I0=0.05,\n",
    "        seed=None,\n",
    "        network_type=\"random\",\n",
    "        n_nodes=100,\n",
    "        p=0.05,\n",
    "        m=2,\n",
    "        collect=True,\n",
    "        strategy_choice_func: str = \"imitate\",\n",
    "        tau: float = 1.0,\n",
    "    ):\n",
    "        super().__init__(seed=seed)\n",
    "\n",
    "        # Build graph\n",
    "        if network_type == \"BA\":\n",
    "            G = nx.barabasi_albert_graph(n_nodes, m, seed=seed)\n",
    "        else:\n",
    "            G = nx.erdos_renyi_graph(n_nodes, p, seed=seed)\n",
    "        self.G = G\n",
    "        self.grid = NetworkGrid(G)\n",
    "        self.schedule = SimultaneousActivation(self)\n",
    "\n",
    "        # parameters\n",
    "        self.a0 = a0\n",
    "        self.beta_I = beta_I\n",
    "        self.b = b\n",
    "        self.g_I = g_I\n",
    "        self.infrastructure = I0\n",
    "        self.step_count = 0\n",
    "        self.strategy_choice_func = strategy_choice_func\n",
    "        self.tau = tau\n",
    "\n",
    "        # initialize node attribute for agent reference\n",
    "        for n in self.G.nodes:\n",
    "            self.G.nodes[n][\"agent\"] = []\n",
    "\n",
    "        # choose initial EV nodes\n",
    "        total_nodes = self.G.number_of_nodes()\n",
    "        k_ev = max(0, min(initial_ev, total_nodes))\n",
    "        ev_nodes = set(self.random.sample(list(self.G.nodes), k_ev))\n",
    "\n",
    "        # create one agent per node\n",
    "        uid = 0\n",
    "        for node in self.G.nodes:\n",
    "            init_strategy = \"C\" if node in ev_nodes else \"D\"\n",
    "            agent = EVAgent(uid, self, init_strategy)\n",
    "            uid += 1\n",
    "            self.schedule.add(agent)\n",
    "            self.grid.place_agent(agent, node)\n",
    "\n",
    "        self.datacollector = None\n",
    "        if collect:\n",
    "            self.datacollector = DataCollector(\n",
    "                model_reporters={\n",
    "                    \"X\": self.get_adoption_fraction,\n",
    "                    \"I\": lambda m: m.infrastructure,\n",
    "                },\n",
    "                agent_reporters={\"strategy\": \"strategy\", \"payoff\": \"payoff\"},\n",
    "            )\n",
    "\n",
    "    def get_adoption_fraction(self):\n",
    "        agents = self.schedule.agents\n",
    "        if not agents:\n",
    "            return 0.0\n",
    "        return sum(1 for a in agents if a.strategy == \"C\") / len(agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4758322",
   "metadata": {},
   "source": [
    "## Model Step Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb712534",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ####################\n",
    "    # Model step function\n",
    "    #\n",
    "    # The step function advances the model by one time step.\n",
    "    # It first advances all agents, then computes the adoption fraction and infrastructure level.\n",
    "    # The infrastructure level is updated based on the adoption fraction and the infrastructure growth rate.\n",
    "    # The updated infrastructure level is clipped to the interval [0, 1].\n",
    "    # Finally, if data collection is enabled, the model and agent data are collected.\n",
    "    #######################\n",
    "    def step(self): \n",
    "        self.schedule.step() # advance all agents\n",
    "        X = self.get_adoption_fraction() # compute adoption fraction after all agents have advanced\n",
    "        I = self.infrastructure # infrastructure level before this step\n",
    "        dI = self.g_I * (X - I) # infrastructure growth rate, impacted by adoption fraction\n",
    "        self.infrastructure = float(min(1.0, max(0.0, I + dI))) # clip infrastructure level to [0, 1]\n",
    "        if self.datacollector is not None:\n",
    "            self.datacollector.collect(self) # collect data at the end of each step\n",
    "        self.step_count += 1 # increment step count after data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fa9dfa",
   "metadata": {},
   "source": [
    "## Set Initial Adopters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ac8d152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Set initial adopters\n",
    "# \n",
    "# Parameters\n",
    "# - model: the EVStagHuntModel instance\n",
    "# - X0_frac: fraction of agents to initially choose EV adoption\n",
    "# - method: method to choose initial adopters (\"random\" or \"degree\")\n",
    "# - seed: random seed for reproducibility\n",
    "# - high: whether to choose high or low degree nodes for \"degree\" method\n",
    "###########################\n",
    "def set_initial_adopters(model, X0_frac, method=\"random\", seed=None, high=True):\n",
    "    \"\"\"Set a fraction of agents to EV adopters using different heuristics.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    agents = model.schedule.agents\n",
    "    n = len(agents)\n",
    "    k = int(round(X0_frac * n))\n",
    "\n",
    "    for a in agents:\n",
    "        a.strategy = \"D\"\n",
    "\n",
    "    if k <= 0:\n",
    "        return\n",
    "\n",
    "    if method == \"random\":\n",
    "        idx = rng.choice(n, size=k, replace=False)\n",
    "        for i in idx:\n",
    "            agents[i].strategy = \"C\"\n",
    "        return\n",
    "\n",
    "    if method == \"degree\":\n",
    "        deg = dict(model.G.degree())\n",
    "        ordered_nodes = sorted(deg.keys(), key=lambda u: deg[u], reverse=high)\n",
    "        chosen = set(ordered_nodes[:k])\n",
    "        for a in agents:\n",
    "            if a.unique_id in chosen:\n",
    "                a.strategy = \"C\"\n",
    "        return\n",
    "\n",
    "    raise ValueError(f\"Unknown method: {method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3870e780",
   "metadata": {},
   "source": [
    "## Ratio Sweep Helpers (Computation-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11b8b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Ratio sweep helpers (computation-only)\n",
    "# -----------------------------\n",
    "#########################\n",
    "#\n",
    "# Run a single network trial\n",
    "# \n",
    "# Parameters\n",
    "# - X0_frac: fraction of agents to initially choose EV adoption\n",
    "# - ratio: payoff ratio between EV and DC agents (a0 = ratio*b - beta_I*I0)\n",
    "# - I0: initial infrastructure level\n",
    "# - beta_I: cost of EV adoption relative to DC (beta_I*I0)\n",
    "# - b: payoff of EV (b)\n",
    "# - g_I: infrastructure growth rate (g_I)\n",
    "# - T: number of time steps to run\n",
    "# - network_type: type of network to generate (\"random\" or \"BA\")\n",
    "# - n_nodes: number of nodes in the network\n",
    "# - p: probability of edge creation in random network\n",
    "# - m: number of edges to attach from a new node to existing nodes in BA network\n",
    "# - seed: random seed for reproducibility\n",
    "# - tol: tolerance for convergence check (default: 1e-3)\n",
    "# - patience: number of steps to wait for convergence (default: 30)\n",
    "\n",
    "def run_network_trial(\n",
    "    X0_frac: float,\n",
    "    ratio: float,\n",
    "    *,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 200,\n",
    "    network_type: str = \"random\",\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    seed: int | None = None,\n",
    "    tol: float = 1e-3,\n",
    "    patience: int = 30,\n",
    "    collect: bool = False,\n",
    "    strategy_choice_func: str = \"imitate\",\n",
    "    tau: float = 1.0,\n",
    ") -> float:\n",
    "    \"\"\"Run a single realisation and return final adoption fraction.\n",
    "\n",
    "    Preserves the intended initial payoff ratio via a0 = ratio*b - beta_I*I0.\n",
    "    Includes basic stability-based early stopping.\n",
    "    \"\"\"\n",
    "    initial_ev = int(round(X0_frac * n_nodes))\n",
    "    a0 = ratio * b - beta_I * I0\n",
    "\n",
    "    model = EVStagHuntModel(\n",
    "        initial_ev=initial_ev,\n",
    "        a0=a0,\n",
    "        beta_I=beta_I,\n",
    "        b=b,\n",
    "        g_I=g_I,\n",
    "        I0=I0,\n",
    "        seed=seed,\n",
    "        network_type=network_type,\n",
    "        n_nodes=n_nodes,\n",
    "        p=p,\n",
    "        m=m,\n",
    "        collect=collect,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    stable_steps = 0\n",
    "    prev_X = None\n",
    "    prev_I = None\n",
    "    for _ in range(T):\n",
    "        model.step()\n",
    "        X = model.get_adoption_fraction()\n",
    "        I = model.infrastructure\n",
    "        if prev_X is not None and prev_I is not None:\n",
    "            if abs(X - prev_X) < tol and abs(I - prev_I) < tol:\n",
    "                stable_steps += 1\n",
    "            else:\n",
    "                stable_steps = 0\n",
    "        prev_X, prev_I = X, I\n",
    "        if X in (0.0, 1.0) and stable_steps >= 10:\n",
    "            break\n",
    "        if stable_steps >= patience:\n",
    "            break\n",
    "\n",
    "    return model.get_adoption_fraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458295d7",
   "metadata": {},
   "source": [
    "# Compute Final Mean Adoption Fraction vs Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86201ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Compute final mean adoption fraction vs ratio\n",
    "# \n",
    "##########################\n",
    "def final_mean_adoption_vs_ratio(\n",
    "    X0_frac: float,\n",
    "    ratio_values: Iterable[float],\n",
    "    *,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 200,\n",
    "    network_type: str = \"random\",\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"imitate\",\n",
    "    tau: float = 1.0,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute mean final adoption across a sweep of ratio values.\n",
    "\n",
    "    For each ratio, average over `batch_size` trials with jittered `I0` and seeds.\n",
    "    Returns a numpy array of means aligned with `ratio_values` order.\n",
    "    \"\"\"\n",
    "    ratios = list(ratio_values)\n",
    "    means: List[float] = []\n",
    "    for ratio in ratios:\n",
    "        finals: List[float] = []\n",
    "        for _ in range(batch_size):\n",
    "            I0_j = float(np.clip(np.random.normal(loc=I0, scale=init_noise_I), 0.0, 1.0))\n",
    "            seed_j = np.random.randint(0, 2**31 - 1)\n",
    "            x_star = run_network_trial(\n",
    "                X0_frac,\n",
    "                ratio,\n",
    "                I0=I0_j,\n",
    "                beta_I=beta_I,\n",
    "                b=b,\n",
    "                g_I=g_I,\n",
    "                T=T,\n",
    "                network_type=network_type,\n",
    "                n_nodes=n_nodes,\n",
    "                p=p,\n",
    "                m=m,\n",
    "                seed=seed_j,\n",
    "                collect=False,\n",
    "                strategy_choice_func=strategy_choice_func,\n",
    "                tau=tau,\n",
    "            )\n",
    "            finals.append(x_star)\n",
    "        means.append(float(np.mean(finals)))\n",
    "    return np.asarray(means, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8346e501",
   "metadata": {},
   "source": [
    "## Compute heatmap rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07023666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Compute heatmap row for a fixed ratio\n",
    "# \n",
    "##########################\n",
    "def _row_for_ratio_task(args: Dict) -> np.ndarray:\n",
    "    \"\"\"Top-level worker to compute one heatmap row for a fixed ratio.\n",
    "\n",
    "    Returns an array of mean final adoption across provided X0_values.\n",
    "    \"\"\"\n",
    "    ratio = args[\"ratio\"]\n",
    "    X0_values = args[\"X0_values\"]\n",
    "    I0 = args[\"I0\"]\n",
    "    beta_I = args[\"beta_I\"]\n",
    "    b = args[\"b\"]\n",
    "    g_I = args[\"g_I\"]\n",
    "    T = args[\"T\"]\n",
    "    network_type = args[\"network_type\"]\n",
    "    n_nodes = args[\"n_nodes\"]\n",
    "    p = args[\"p\"]\n",
    "    m = args[\"m\"]\n",
    "    batch_size = args[\"batch_size\"]\n",
    "    init_noise_I = args[\"init_noise_I\"]\n",
    "    strategy_choice_func = args[\"strategy_choice_func\"]\n",
    "    tau = args[\"tau\"]\n",
    "\n",
    "    row = np.empty(len(X0_values), dtype=float)\n",
    "    for j, X0 in enumerate(X0_values):\n",
    "        finals: List[float] = []\n",
    "        for _ in range(batch_size):\n",
    "            I0_j = float(np.clip(np.random.normal(loc=I0, scale=init_noise_I), 0.0, 1.0))\n",
    "            seed_j = np.random.randint(0, 2**31 - 1)\n",
    "            x_star = run_network_trial(\n",
    "                X0_frac=X0,\n",
    "                ratio=ratio,\n",
    "                I0=I0_j,\n",
    "                beta_I=beta_I,\n",
    "                b=b,\n",
    "                g_I=g_I,\n",
    "                T=T,\n",
    "                network_type=network_type,\n",
    "                n_nodes=n_nodes,\n",
    "                p=p,\n",
    "                m=m,\n",
    "                seed=seed_j,\n",
    "                collect=False,\n",
    "                strategy_choice_func=strategy_choice_func,\n",
    "                tau=tau,\n",
    "            )\n",
    "            finals.append(x_star)\n",
    "        row[j] = float(np.mean(finals))\n",
    "    return row\n",
    "\n",
    "    \n",
    "#########################\n",
    "#\n",
    "# Compute heatmap matrix for phase sweep\n",
    "# \n",
    "##########################\n",
    "def phase_sweep_X0_vs_ratio(\n",
    "    X0_values: Iterable[float],\n",
    "    ratio_values: Iterable[float],\n",
    "    *,\n",
    "    I0: float = 0.05,\n",
    "    beta_I: float = 2.0,\n",
    "    b: float = 1.0,\n",
    "    g_I: float = 0.05,\n",
    "    T: int = 250,\n",
    "    network_type: str = \"BA\",\n",
    "    n_nodes: int = 120,\n",
    "    p: float = 0.05,\n",
    "    m: int = 2,\n",
    "    batch_size: int = 16,\n",
    "    init_noise_I: float = 0.04,\n",
    "    strategy_choice_func: str = \"logit\",\n",
    "    tau: float = 1.0,\n",
    "    max_workers: int | None = None,\n",
    "    backend: str = \"process\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute a heatmap matrix of mean final adoption X* over (X0, ratio).\n",
    "\n",
    "    Returns an array of shape (len(ratio_values), len(X0_values)) aligned with\n",
    "    the provided orders. Rows correspond to ratios; columns to X0 values.\n",
    "    \"\"\"\n",
    "    X0_values = list(X0_values)\n",
    "    ratio_values = list(ratio_values)\n",
    "    X_final = np.zeros((len(ratio_values), len(X0_values)), dtype=float)\n",
    "\n",
    "    # Prepare tasks per ratio\n",
    "    tasks: List[Dict] = []\n",
    "    for ratio in ratio_values:\n",
    "        tasks.append({\n",
    "            \"ratio\": ratio,\n",
    "            \"X0_values\": X0_values,\n",
    "            \"I0\": I0,\n",
    "            \"beta_I\": beta_I,\n",
    "            \"b\": b,\n",
    "            \"g_I\": g_I,\n",
    "            \"T\": T,\n",
    "            \"network_type\": network_type,\n",
    "            \"n_nodes\": n_nodes,\n",
    "            \"p\": p,\n",
    "            \"m\": m,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"init_noise_I\": init_noise_I,\n",
    "            \"strategy_choice_func\": strategy_choice_func,\n",
    "            \"tau\": tau,\n",
    "        })\n",
    "\n",
    "    if max_workers is None:\n",
    "        try:\n",
    "            max_workers = os.cpu_count() or 1\n",
    "        except Exception:\n",
    "            max_workers = 1\n",
    "\n",
    "    Executor = ProcessPoolExecutor if backend == \"process\" and max_workers > 1 else ThreadPoolExecutor\n",
    "    if max_workers > 1:\n",
    "        with Executor(max_workers=max_workers) as ex:\n",
    "            futures = [ex.submit(_row_for_ratio_task, args) for args in tasks]\n",
    "            for i, fut in enumerate(futures):\n",
    "                row = fut.result()\n",
    "                X_final[i, :] = row\n",
    "    else:\n",
    "        for i, args in enumerate(tasks):\n",
    "            row = _row_for_ratio_task(args)\n",
    "            X_final[i, :] = row\n",
    "\n",
    "    return X_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5bacbf",
   "metadata": {},
   "source": [
    "# Trial runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "228e8799",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Trial runner\n",
    "# -----------------------------\n",
    "\n",
    "def run_timeseries_trial(\n",
    "    T: int = 200,\n",
    "    scenario_kwargs: Optional[Dict] = None,\n",
    "    seed: Optional[int] = None,\n",
    "    policy: Optional[Callable] = None,\n",
    "    strategy_choice_func: str = \"imitate\",\n",
    "    tau: float = 1.0,\n",
    ") -> Tuple[np.ndarray, np.ndarray, pd.DataFrame]:\n",
    "    \"\"\"Run a single simulation and return X(t), I(t), and the model dataframe.\"\"\"\n",
    "\n",
    "    scenario = {\n",
    "        # Either provide `ratio` to pin the initial a_I/b, or explicit `a0`.\n",
    "        # Defaults here mirror the classroom-friendly values.\n",
    "        # If `ratio` is present, we compute `a0 = ratio*b - beta_I*I0`.\n",
    "        \"a0\": 2.0,\n",
    "        \"ratio\": None,\n",
    "        \"beta_I\": 3.0,\n",
    "        \"b\": 1.0,\n",
    "        \"g_I\": 0.1,\n",
    "        \"I0\": 0.05,\n",
    "        \"network_type\": \"random\",\n",
    "        \"n_nodes\": 100,\n",
    "        \"p\": 0.05,\n",
    "        \"m\": 2,\n",
    "        \"collect\": True,\n",
    "        \"X0_frac\": 0.0,\n",
    "        \"init_method\": \"random\",\n",
    "    }\n",
    "    if scenario_kwargs:\n",
    "        scenario.update(scenario_kwargs)\n",
    "\n",
    "    # Compute a0 from ratio if provided to preserve initial payoff ratio\n",
    "    a0_for_model = scenario[\"a0\"]\n",
    "    if scenario.get(\"ratio\") is not None:\n",
    "        a0_for_model = float(scenario[\"ratio\"]) * float(scenario[\"b\"]) - float(scenario[\"beta_I\"]) * float(scenario[\"I0\"])\n",
    "\n",
    "    model = EVStagHuntModel(\n",
    "        a0=a0_for_model,\n",
    "        beta_I=scenario[\"beta_I\"],\n",
    "        b=scenario[\"b\"],\n",
    "        g_I=scenario[\"g_I\"],\n",
    "        I0=scenario[\"I0\"],\n",
    "        seed=seed,\n",
    "        network_type=scenario[\"network_type\"],\n",
    "        n_nodes=scenario[\"n_nodes\"],\n",
    "        p=scenario[\"p\"],\n",
    "        m=scenario[\"m\"],\n",
    "        collect=True,\n",
    "        strategy_choice_func=strategy_choice_func,\n",
    "        tau=tau,\n",
    "    )\n",
    "\n",
    "    if scenario.get(\"X0_frac\", 0.0) > 0.0:\n",
    "        set_initial_adopters(\n",
    "            model,\n",
    "            scenario[\"X0_frac\"],\n",
    "            method=scenario.get(\"init_method\", \"random\"),\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "    for t in range(T):\n",
    "        if policy is not None:\n",
    "            policy(model, t)\n",
    "        model.step()\n",
    "\n",
    "    df = model.datacollector.get_model_vars_dataframe().copy()\n",
    "    return df[\"X\"].to_numpy(), df[\"I\"].to_numpy(), df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
